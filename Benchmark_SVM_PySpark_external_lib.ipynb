{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Semiparametric SVM training using subgradients in Spark **\n",
    "\n",
    "#### bla, bla, bla. \n",
    "\n",
    "#### We will benchmark the algorithms with data files from UCI:\n",
    "\n",
    "* **Ripley**, the Ripley dataset\n",
    "* **Kwok**, the Kwok dataset\n",
    "* **Twonorm**, the Twonorm dataset\n",
    "* **Waveform**, the Waveform dataset\n",
    "* **Covertype**, the Covertype dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Adult\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/2.7.9/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/core/fromnumeric.py:2652: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  VisibleDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset = Adult, modelo = SGMA_IRWLS, kfold = 0, Niter = 300, NC = 50\n",
      "Centroid 1 : Taking candidates, Evaluating ED, Max ED: 824.264119271 , Updating Matrices Time 2.60042881966\n",
      "Centroid 2 : Taking candidates, Evaluating ED, Max ED: 4.29576321751 , Updating Matrices Time 4.10725998878\n",
      "Centroid 3 : Taking candidates, Evaluating ED, Max ED: 0.916127629346 , Updating Matrices Time 4.45559406281\n",
      "Centroid 4 : Taking candidates, Evaluating ED, Max ED: 0.31179052612 , Updating Matrices Time 4.18284606934\n",
      "Centroid 5 : Taking candidates, Evaluating ED, Max ED: 0.175716703015 , Updating Matrices Time 3.56374716759\n",
      "Centroid 6 : Taking candidates, Evaluating ED, Max ED: 0.118047398316 , Updating Matrices Time 3.47891998291\n",
      "Centroid 7 : Taking candidates, Evaluating ED, Max ED: 0.097969189982 , Updating Matrices Time 3.44574093819\n",
      "Centroid 8 : Taking candidates, Evaluating ED, Max ED: 0.0768576888127 , Updating Matrices Time 3.65816783905\n",
      "Centroid 9 : Taking candidates, Evaluating ED, Max ED: 0.103096548669 , Updating Matrices Time 3.73713493347\n",
      "Centroid 10 : Taking candidates, Evaluating ED, Max ED: 0.0539815436161 , Updating Matrices Time 3.63562297821\n",
      "Centroid 11 : Taking candidates, Evaluating ED, Max ED: 0.0402128680325 , Updating Matrices Time 3.72818398476\n",
      "Centroid 12 : Taking candidates, Evaluating ED, Max ED: 0.04807610542 , Updating Matrices Time 3.56276702881\n",
      "Centroid 13 : Taking candidates, Evaluating ED, Max ED: 0.0423265878594 , Updating Matrices Time 3.57136893272\n",
      "Centroid 14 : Taking candidates, Evaluating ED, Max ED: 0.038136346495 , Updating Matrices Time 3.67243409157\n",
      "Centroid 15 : Taking candidates, Evaluating ED, Max ED: 0.0306916987127 , Updating Matrices Time 3.70936608315\n",
      "Centroid 16 : Taking candidates, Evaluating ED, Max ED: 0.0346599729422 , Updating Matrices Time 3.74382185936\n",
      "Centroid 17 : Taking candidates, Evaluating ED, Max ED: 0.0275878209506 , Updating Matrices Time 3.81574392319\n",
      "Centroid 18 : Taking candidates, Evaluating ED, Max ED: 0.0245742748841 , Updating Matrices Time 3.63044905663\n",
      "Centroid 19 : Taking candidates, Evaluating ED, Max ED: 0.0204079895161 , Updating Matrices Time 4.56516003609\n",
      "Centroid 20 : Taking candidates, Evaluating ED, Max ED: 0.027139584361 , Updating Matrices Time 3.81868886948\n",
      "Centroid 21 : Taking candidates, Evaluating ED, Max ED: 0.0197057622629 , Updating Matrices Time 4.1755259037\n",
      "Centroid 22 : Taking candidates, Evaluating ED, Max ED: 0.0177937998899 , Updating Matrices Time 3.52048707008\n",
      "Centroid 23 : Taking candidates, Evaluating ED, Max ED: 0.0117255275117 , Updating Matrices Time 3.57644295692\n",
      "Centroid 24 : Taking candidates, Evaluating ED, Max ED: 0.0179393841288 , Updating Matrices Time 3.61291408539\n",
      "Centroid 25 : Taking candidates, Evaluating ED, Max ED: 0.0115870258937 , Updating Matrices Time 4.17059707642\n",
      "Centroid 26 : Taking candidates, Evaluating ED, Max ED: 0.0124080295462 , Updating Matrices Time 4.08630609512\n",
      "Centroid 27 : Taking candidates, Evaluating ED, Max ED: 0.00842453248103 , Updating Matrices Time 4.08725214005\n",
      "Centroid 28 : Taking candidates, Evaluating ED, Max ED: 0.0117527603486 , Updating Matrices Time 4.09095096588\n",
      "Centroid 29 : Taking candidates, Evaluating ED, Max ED: 0.0134898899417 , Updating Matrices Time 4.24210000038\n",
      "Centroid 30 : Taking candidates, Evaluating ED, Max ED: 0.00795329408916 , Updating Matrices Time 4.18695521355\n",
      "Centroid 31 : Taking candidates, Evaluating ED, Max ED: 0.00989988793702 , Updating Matrices Time 4.64611697197\n",
      "Centroid 32 : Taking candidates, Evaluating ED, Max ED: 0.00959656113742 , Updating Matrices Time 4.04517912865\n",
      "Centroid 33 : Taking candidates, Evaluating ED, Max ED: 0.00609220468366 , Updating Matrices Time 4.06808495522\n",
      "Centroid 34 : Taking candidates, Evaluating ED, Max ED: 0.00500848283148 , Updating Matrices Time 4.19445705414\n",
      "Centroid 35 : Taking candidates, Evaluating ED, Max ED: 0.00488605412045 , Updating Matrices Time 4.0526599884\n",
      "Centroid 36 : Taking candidates, Evaluating ED, Max ED: 0.00440946527223 , Updating Matrices Time 4.19976210594\n",
      "Centroid 37 : Taking candidates, Evaluating ED, Max ED: 0.00577665073459 , Updating Matrices Time 4.31785106659\n",
      "Centroid 38 : Taking candidates, Evaluating ED, Max ED: 0.00700185326985 , Updating Matrices Time 4.76384711266\n",
      "Centroid 39 : Taking candidates, Evaluating ED, Max ED: 0.00444557261731 , Updating Matrices Time 4.27750396729\n",
      "Centroid 40 : Taking candidates, Evaluating ED, Max ED: 0.00434611484886 , Updating Matrices Time 4.39431190491\n",
      "Centroid 41 : Taking candidates, Evaluating ED, Max ED: 0.0030492837206 , Updating Matrices Time 4.31896209717\n",
      "Centroid 42 : Taking candidates, Evaluating ED, Max ED: 0.002605857718 , Updating Matrices Time 4.2793648243\n",
      "Centroid 43 : Taking candidates, Evaluating ED, Max ED: 0.00265238271934 , Updating Matrices Time 4.18121695518\n",
      "Centroid 44 : Taking candidates, Evaluating ED, Max ED: 0.00408046426101 , Updating Matrices Time 4.82309913635\n",
      "Centroid 45 : Taking candidates, Evaluating ED, Max ED: 0.00237697448415 , Updating Matrices Time 4.4079310894\n",
      "Centroid 46 : Taking candidates, Evaluating ED, Max ED: 0.00344439853644 , Updating Matrices Time 4.23515892029\n",
      "Centroid 47 : Taking candidates, Evaluating ED, Max ED: 0.00204431286976 , Updating Matrices Time 4.42479610443\n",
      "Centroid 48 : Taking candidates, Evaluating ED, Max ED: 0.00176535601347 , Updating Matrices Time 4.21107006073\n",
      "Centroid 49 : Taking candidates, Evaluating ED, Max ED: 0.00165939636652 , Updating Matrices Time 4.20792007446\n",
      "Centroid 50 : Taking candidates, Evaluating ED, Max ED: 0.00105749962532 , Updating Matrices Time 4.29966688156\n",
      "Iteration 1 : DeltaW/W inf , Iteration Time 3.09999418259\n",
      "Iteration 2 : DeltaW/W 0.368873174672 , Iteration Time 0.942849159241\n",
      "Iteration 3 : DeltaW/W 0.174547212031 , Iteration Time 0.839074134827\n",
      "Iteration 4 : DeltaW/W 0.106898765436 , Iteration Time 0.941772937775\n",
      "Iteration 5 : DeltaW/W 0.133377459344 , Iteration Time 1.05315494537\n",
      "Iteration 6 : DeltaW/W 0.0956743223698 , Iteration Time 0.757735013962\n",
      "Iteration 7 : DeltaW/W 0.0747108931801 , Iteration Time 0.80802989006\n",
      "Iteration 8 : DeltaW/W 0.0640805176524 , Iteration Time 0.711505889893\n",
      "Iteration 9 : DeltaW/W 0.0588701724174 , Iteration Time 0.753468990326\n",
      "Iteration 10 : DeltaW/W 0.0502552707978 , Iteration Time 0.823668003082\n",
      "Iteration 11 : DeltaW/W 0.0510531178976 , Iteration Time 0.705417871475\n",
      "Iteration 12 : DeltaW/W 0.0475329766179 , Iteration Time 0.712731122971\n",
      "Iteration 13 : DeltaW/W 0.0454490093461 , Iteration Time 0.769870996475\n",
      "Iteration 14 : DeltaW/W 0.0306528449166 , Iteration Time 0.729830026627\n",
      "Iteration 15 : DeltaW/W 0.037652508476 , Iteration Time 0.901165962219\n",
      "Iteration 16 : DeltaW/W 0.0250072515687 , Iteration Time 0.735023975372\n",
      "Iteration 17 : DeltaW/W 0.026230198312 , Iteration Time 0.68065905571\n",
      "Iteration 18 : DeltaW/W 0.0212846346418 , Iteration Time 0.726660966873\n",
      "Iteration 19 : DeltaW/W 0.0217786332808 , Iteration Time 0.728407144547\n",
      "Iteration 20 : DeltaW/W 0.0145240643998 , Iteration Time 0.676565885544\n",
      "Iteration 21 : DeltaW/W 0.0107893302067 , Iteration Time 0.673715829849\n",
      "Iteration 22 : DeltaW/W 0.00904344736732 , Iteration Time 0.697500944138\n",
      "Iteration 23 : DeltaW/W 0.00638199134441 , Iteration Time 0.75776386261\n",
      "Iteration 24 : DeltaW/W 0.00646132268427 , Iteration Time 0.905904054642\n",
      "Iteration 25 : DeltaW/W 0.00438892264554 , Iteration Time 0.686151981354\n",
      "Iteration 26 : DeltaW/W 0.00383657525849 , Iteration Time 0.689554929733\n",
      "Iteration 27 : DeltaW/W 0.00359893019473 , Iteration Time 0.771398067474\n",
      "Iteration 28 : DeltaW/W 0.00300640315805 , Iteration Time 0.715826034546\n",
      "Iteration 29 : DeltaW/W 0.00274257389002 , Iteration Time 0.747478961945\n",
      "Iteration 30 : DeltaW/W 0.00242095789146 , Iteration Time 0.660061120987\n",
      "Iteration 31 : DeltaW/W 0.00209097241711 , Iteration Time 0.711186170578\n",
      "Iteration 32 : DeltaW/W 0.00195206580682 , Iteration Time 0.703935146332\n",
      "Iteration 33 : DeltaW/W 0.00171750348833 , Iteration Time 0.699125051498\n",
      "Iteration 34 : DeltaW/W 0.0015513276956 , Iteration Time 0.680516004562\n",
      "Iteration 35 : DeltaW/W 0.00139309237545 , Iteration Time 0.660701990128\n",
      "Iteration 36 : DeltaW/W 0.00125975272241 , Iteration Time 0.707259178162\n",
      "Iteration 37 : DeltaW/W 0.00115105310397 , Iteration Time 0.687336206436\n",
      "Iteration 38 : DeltaW/W 0.0010336553777 , Iteration Time 0.743426084518\n",
      "Dataset = Adult, modelo = SGMA_IRWLS, kfold = 0, Niter = 300, NC = 50\n",
      "AUCtr = 0.901332, AUCval = 0.901332, AUCtst = 0.899831\n",
      "Elapsed minutes = 5.215199\n",
      "Number of Support Vectors = 11639\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: './results/dataset_4_modelo_SGMA_IRWLS_NC_50_Niter_300_kfold_0.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d636ae54c0d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m                         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Number of Support Vectors = %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNSVs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m                             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mauc_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc_tst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexe_time\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: './results/dataset_4_modelo_SGMA_IRWLS_NC_50_Niter_300_kfold_0.pkl'"
     ]
    }
   ],
   "source": [
    "# definir variable de usuario y ejecutar condicionalmente cualquier código que dependa del contexto de ejecución\n",
    "# no borrar código, simplemente ejecutarlo o no con un \"if\"\n",
    "# cada uno mantiene actualizada su parte del \"if\" y no toca la del otro\n",
    "\n",
    "#user = 'navia'\n",
    "user = 'roberto'\n",
    "\n",
    "#modelo = 'hybrid' \n",
    "#modelo = 'kernelgrad' \n",
    "overwrite_results = True # overwrites results even when the result file exists, skips the execution otherwise\n",
    "\n",
    "if user == 'roberto':\n",
    "    # definir sc\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "    from pyspark import SparkConf, SparkContext\n",
    "    conf = (SparkConf().setMaster(\"local[4]\").setAppName(\"My app\").set(\"spark.executor.memory\", \"2g\"))\n",
    "    sc = SparkContext(conf = conf)\n",
    "    import common.lib.svm_utils as SVM_UTILS\n",
    "    import numpy as np\n",
    "    from pyspark.mllib.regression import LabeledPoint\n",
    "    import pickle\n",
    "    from math import sqrt\n",
    "    %matplotlib inline\n",
    "    # Se importan las funciones\n",
    "    from pyspark.mllib.util import MLUtils\n",
    "    from common.lib.IRWLSUtils import *\n",
    "    Npartitions = 12\n",
    "    Samplefraction = 0.05\n",
    "\n",
    "if user == 'navia':\n",
    "    sc.addPyFile(\"file:///export/usuarios01/navia/spark/SVM_spark/common/lib/svm_utils.py\")\n",
    "    sc.addPyFile(\"file:///export/usuarios01/navia/spark/SVM_spark/common/lib/IRWLSUtils.py\")   \n",
    "    sc.addPyFile(\"file:///export/usuarios01/navia/spark/SVM_spark/common/lib/KernelUtils.py\")   \n",
    "    sc.addPyFile(\"file:///export/usuarios01/navia/spark/SVM_spark/common/lib/ResultsUtils.py\")   \n",
    "    sc.addPyFile(\"file:///export/usuarios01/navia/spark/SVM_spark/common/lib/SGMAUtils.py\")   \n",
    "    import svm_utils as SVM_UTILS\n",
    "    #from SGMAUtils import SGMA\n",
    "    from IRWLSUtils import loadFile, train_SGMA_IRWLS, train_random_IRWLS, train_hybrid_SGMA_IRWLS, train_kmeans_IRWLS, train_Ballanced_SGMA_IRWLS\n",
    "    #from ResultsUtils import compute_AUCs\n",
    "    import numpy as np\n",
    "    from pyspark.mllib.regression import LabeledPoint\n",
    "    from pyspark.mllib.linalg import SparseVector, DenseVector\n",
    "    import pickle\n",
    "    from math import sqrt\n",
    "    from pyspark.mllib.util import MLUtils\n",
    "    %matplotlib inline\n",
    "    Npartitions = -99 # cuando es menor que 0, no se aplica y se deja libre\n",
    "    Samplefraction = 0.05\n",
    "\n",
    "# 0 = Ripley\n",
    "# 1 = Kwok\n",
    "# 2 = Twonorm\n",
    "# 3 = Waveform\n",
    "# 4 = Adult\n",
    "# 5 = SUSY\n",
    "# 6 = KddCup1999, requiere preprocesado...\n",
    "# 7 = Higgs\n",
    "\n",
    "datasets = [0, 1, 2, 3, 4, 5, 6]\n",
    "folds = [0, 1, 2, 3, 4]\n",
    "dataset_names = ['Ripley', 'Kwok', 'Twonorm', 'Waveform', 'Adult', 'Susy', 'KddCup1999', 'Higgs']\n",
    "Niters = [50, 100, 200]\n",
    "NCs = [5, 10, 25, 50, 100, 200]\n",
    "modelos = ['hybridgrad', 'kernelgrad', 'SGMA_IRWLS', 'LinearSVM', 'Logistic', 'random_IRWLS', 'hybrid_IRWLS', 'Kmeans_IRWLS', 'balanced_SGMA_IRWLS']\n",
    "\n",
    "datasets = [4]\n",
    "folds = [0]\n",
    "modelos = ['SGMA_IRWLS']\n",
    "Niters = [300]\n",
    "NCs = [50]\n",
    "Samplefraction = 0.05\n",
    "fsigma = 1.0\n",
    "\n",
    "for modelo in modelos:\n",
    "    for kdataset in datasets:\n",
    "        for kfold in folds:\n",
    "            for Niter in Niters:\n",
    "                C = 100.0\n",
    "                name_dataset = dataset_names[kdataset]\n",
    "                \n",
    "                #####################################################################################################################\n",
    "                # DATA LOADING: the result of this part must always be three RDDs for train, val and test, containing labelled points.\n",
    "                #####################################################################################################################\n",
    "                if kdataset in [0, 1, 2, 3]: # loading from .mat\n",
    "                    x_tr, y_tr, x_val, y_val, x_tst, y_tst = SVM_UTILS.load_data(kdataset, kfold)\n",
    "                    NI = x_tr.shape[1]\n",
    "                    sigma = fsigma * sqrt(NI)\n",
    "                    \n",
    "                    #Npartitions = 65\n",
    "                    \n",
    "                    if Npartitions > 0:\n",
    "                        XtrRDD = sc.parallelize(np.hstack((y_tr, x_tr)), Npartitions).map(lambda x: LabeledPoint(x[0], x[1:]))\n",
    "                        XvalRDD = sc.parallelize(np.hstack((y_val, x_val)), Npartitions).map(lambda x: LabeledPoint(x[0], x[1:]))\n",
    "                        XtstRDD = sc.parallelize(np.hstack((y_tst, x_tst)), Npartitions).map(lambda x: LabeledPoint(x[0], x[1:]))\n",
    "                    else:\n",
    "                        XtrRDD = sc.parallelize(np.hstack((y_tr, x_tr))).map(lambda x: LabeledPoint(x[0], x[1:]))\n",
    "                        XvalRDD = sc.parallelize(np.hstack((y_val, x_val))).map(lambda x: LabeledPoint(x[0], x[1:]))\n",
    "                        XtstRDD = sc.parallelize(np.hstack((y_tst, x_tst))).map(lambda x: LabeledPoint(x[0], x[1:]))\n",
    "                       \n",
    "                        \n",
    "                if kdataset in [4]: # loading libsvm format\n",
    "                    if kdataset == 4:\n",
    "                        dimensions = 123                       \n",
    "                    print \"Loading \" + name_dataset\n",
    "                    XtrRDD = loadFile('./data/' + name_dataset.lower() + '_train',sc,dimensions,Npartitions)\n",
    "                    XvalRDD = loadFile('./data/' + name_dataset.lower() + '_val',sc,dimensions,Npartitions)\n",
    "                    XtstRDD = loadFile('./data/' + name_dataset.lower() + '_test',sc,dimensions,Npartitions)\n",
    "                    sigma = fsigma * np.sqrt(dimensions)\n",
    "                    \n",
    "                if kdataset in [5]: # loading in libsvm format and splitting RDD\n",
    "                    if kdataset == 5:\n",
    "                        filename = 'file:///export/usuarios01/navia/spark/SVM_spark/data/SUSY.txt'\n",
    "                        #filename = 'file:///export/usuarios01/navia/spark/SVM_spark/data/minisusy.txt'\n",
    "                        dimensions = 18\n",
    "                    rawdata = MLUtils.loadLibSVMFile(sc, filename)\n",
    "                    # Labelled points are sparse, we map the values\n",
    "                    rawdata = rawdata.map(lambda x: LabeledPoint(x.label, DenseVector((x.features).toArray())  ))\n",
    "                    XtrRDD, XvalRDD, XtstRDD = rawdata.randomSplit(weights=[0.7, 0.1, 0.2], seed=1234)\n",
    "                    sigma = fsigma * np.sqrt(dimensions)\n",
    "\n",
    "                if kdataset in [7]: # loading as text, transforming to labelledpoint and splitting RDD into train, val and test\n",
    "                    if kdataset == 7:\n",
    "                        \n",
    "                        filename = 'file:///export/g2pi/SPARK/data/higgs_tr.txt'\n",
    "                        XtrRDD = sc.textFile(filename)\n",
    "                        XtrRDD = XtrRDD.map(SVM_UTILS.text2labeled)\n",
    "\n",
    "                        filename = 'file:///export/g2pi/SPARK/data/higgs_val.txt'\n",
    "                        XvalRDD = sc.textFile(filename)\n",
    "                        XvalRDD = XvalRDD.map(SVM_UTILS.text2labeled)\n",
    "                                                \n",
    "                        filename = 'file:///export/g2pi/SPARK/data/higgs_tst.txt'\n",
    "                        XtstRDD = sc.textFile(filename)\n",
    "                        XtstRDD = XtstRDD.map(SVM_UTILS.text2labeled)\n",
    "                        sigma = fsigma * np.sqrt(len(XtstRDD.take(1)[0].features))\n",
    "\n",
    "                        XtrRDD.cache()\n",
    "                        XvalRDD.cache()\n",
    "                        XtstRDD.cache()\n",
    "\n",
    "                #################################   END LOADING DATA ##########################################################\n",
    "                \n",
    "                for NC in NCs:\n",
    "                    print \"Dataset = %s, modelo = %s, kfold = %d, Niter = %d, NC = %d\" % (name_dataset, modelo, kfold, Niter, NC)\n",
    "                    filename = './results/dataset_' + str(kdataset) + '_modelo_' + modelo + '_NC_' + str(NC) + '_Niter_' + str(Niter) + '_kfold_' + str(kfold) + '.pkl'\n",
    "                    #print XtrRDD.count()\n",
    "                    #print XvalRDD.count()\n",
    "                    #print XtstRDD.count()\n",
    "                    \n",
    "                    #import code\n",
    "                    #code.interact(local=locals())\n",
    "                    try:\n",
    "                        f = open(filename,'r')\n",
    "                        f.close()\n",
    "                        file_exists = True\n",
    "                    except:\n",
    "                        file_exists = False\n",
    "                        pass\n",
    "                    execute = False\n",
    "                    if file_exists:\n",
    "                        if overwrite_results:\n",
    "                            execute = True\n",
    "                    else:\n",
    "                        execute = True                                  \n",
    "                    if execute:\n",
    "\n",
    "                        if modelo == 'hybridgrad':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time = SVM_UTILS.train_hybridSVM(XtrRDD, XvalRDD, XtstRDD, sigma, C, NC, Niter, Samplefraction)\n",
    "\n",
    "                        if modelo == 'kernelgrad':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time = SVM_UTILS.train_kernelgrad(XtrRDD, XvalRDD, XtstRDD, sigma, C, NC, Niter, Samplefraction)\n",
    "\n",
    "                        if modelo == 'SGMA_IRWLS':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time, NSVs = train_SGMA_IRWLS(XtrRDD, XvalRDD, XtstRDD, sigma, C, NC, Niter)\n",
    "\n",
    "                        if modelo == 'LinearSVM':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time = SVM_UTILS.train_linear_SVM(XtrRDD, XvalRDD, XtstRDD)\n",
    "                                                \n",
    "                        if modelo == 'Logistic':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time = SVM_UTILS.train_logistic(XtrRDD, XvalRDD, XtstRDD)\n",
    "\n",
    "                        if modelo == 'random_IRWLS':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time, NSVs = train_random_IRWLS(XtrRDD, XvalRDD, XtstRDD, sigma, C, NC, Niter)\n",
    "\n",
    "                        if modelo == 'hybrid_IRWLS':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time, NSVs = train_hybrid_SGMA_IRWLS(XtrRDD, XvalRDD, XtstRDD, sigma, C, NC, Niter)\n",
    "\n",
    "                        if modelo == 'Kmeans_IRWLS':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time, NSVs = train_kmeans_IRWLS(XtrRDD, XvalRDD, XtstRDD, sigma, C, NC, Niter)\n",
    "\n",
    "                        if modelo == 'balanced_SGMA_IRWLS':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time, NSVs = train_Ballanced_SGMA_IRWLS(XtrRDD, XvalRDD, XtstRDD, sigma, C, NC, Niter)\n",
    "\n",
    "                        print \"Dataset = %s, modelo = %s, kfold = %d, Niter = %d, NC = %d\" % (name_dataset, modelo, kfold, Niter, NC)\n",
    "                        print \"AUCtr = %f, AUCval = %f, AUCtst = %f\" % (auc_tr, auc_val, auc_tst)\n",
    "                        print \"Elapsed minutes = %f\" % (exe_time / 60.0)\n",
    "                        print \"Number of Support Vectors = %d\" % (NSVs)\n",
    "\n",
    "                        with open(filename, 'w') as f:\n",
    "                            pickle.dump([auc_tr, auc_val, auc_tst, exe_time], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fsigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
