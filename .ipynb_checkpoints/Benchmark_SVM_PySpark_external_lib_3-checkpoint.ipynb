{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Semiparametric SVM training using subgradients in Spark **\n",
    "\n",
    "#### bla, bla, bla. \n",
    "\n",
    "#### We will benchmark the algorithms with data files from UCI:\n",
    "\n",
    "* **Ripley**, the Ripley dataset\n",
    "* **Kwok**, the Kwok dataset\n",
    "* **Twonorm**, the Twonorm dataset\n",
    "* **Waveform**, the Waveform dataset\n",
    "* **Covertype**, the Covertype dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset = Susy, modelo = SGMA_IRWLS, kfold = 0, Niter = 150, NC = 50\n",
      "Mapping labels to (-1, 1)...\n",
      "Centroid 1 : Taking candidates, Evaluating ED, Max ED: 287.332110383 , Updating Matrices Time 92.0096747875\n",
      "Centroid 2 : Taking candidates, Evaluating ED, Max ED: 105.364924898 , Updating Matrices Time 88.6006610394\n",
      "Centroid 3 : Taking candidates, Evaluating ED, Max ED: 42.2872591551 , Updating Matrices Time 87.9730811119\n",
      "Centroid 4 : Taking candidates, Evaluating ED, Max ED: 38.2705726639 , Updating Matrices Time 101.198817015\n",
      "Centroid 5 : Taking candidates, Evaluating ED, Max ED: 42.4912086772 , Updating Matrices Time 88.0436518192\n",
      "Centroid 6 : Taking candidates, Evaluating ED, Max ED: 32.8960704908 , Updating Matrices Time 97.5236308575\n",
      "Centroid 7 : Taking candidates, Evaluating ED, Max ED: 25.7783201583 , Updating Matrices Time 104.596613169\n",
      "Centroid 8 : Taking candidates, Evaluating ED, Max ED: 26.3479689178 , Updating Matrices Time 96.524559021\n",
      "Centroid 9 : Taking candidates, Evaluating ED, Max ED: 29.6976708784 , Updating Matrices Time 93.5555310249\n",
      "Centroid 10 : Taking candidates, Evaluating ED, Max ED: 21.9715429318 , Updating Matrices Time 108.259922028\n",
      "Centroid 11 : Taking candidates, Evaluating ED, Max ED: 22.9557236148 , Updating Matrices Time 100.175294876\n",
      "Centroid 12 : Taking candidates, Evaluating ED, Max ED: 22.8895674721 , Updating Matrices Time 103.186228991\n",
      "Centroid 13 : Taking candidates, Evaluating ED, Max ED: 70.6043931804 , Updating Matrices Time 105.112874031\n",
      "Centroid 14 : Taking candidates, Evaluating ED, Max ED: 32.3083403883 , Updating Matrices Time 110.617336035\n",
      "Centroid 15 : Taking candidates, Evaluating ED, Max ED: 30.6975329714 , Updating Matrices Time 119.590072155\n",
      "Centroid 16 : Taking candidates, Evaluating ED, Max ED: 22.3226962457 , Updating Matrices Time 104.645571232\n",
      "Centroid 17 : Taking candidates, Evaluating ED, Max ED: 27.2030117748 , Updating Matrices Time 105.234349012\n",
      "Centroid 18 : Taking candidates, Evaluating ED, Max ED: 24.7719104759 , Updating Matrices Time 110.258202076\n",
      "Centroid 19 : Taking candidates, Evaluating ED, Max ED: 52.8475393214 , Updating Matrices Time 119.505173206\n",
      "Centroid 20 : Taking candidates, Evaluating ED, Max ED: 17.1597959692 , Updating Matrices Time 104.600587845\n",
      "Centroid 21 : Taking candidates, Evaluating ED, Max ED: 24.2486399822 , Updating Matrices Time 127.205587864\n",
      "Centroid 22 : Taking candidates, Evaluating ED, Max ED: 22.2754602607 , Updating Matrices Time 122.308561802\n",
      "Centroid 23 : Taking candidates, Evaluating ED, Max ED: 27.6318293581 , Updating Matrices Time 107.537009001\n",
      "Centroid 24 : Taking candidates, Evaluating ED, Max ED: 32.6429396747 , Updating Matrices Time 114.357964039\n",
      "Centroid 25 : Taking candidates, Evaluating ED, Max ED: 38.6054615198 , Updating Matrices Time 116.174082994\n",
      "Centroid 26 : Taking candidates, Evaluating ED, Max ED: 17.3337858987 , Updating Matrices Time 127.836195946\n",
      "Centroid 27 : Taking candidates, Evaluating ED, Max ED: 22.3150730696 , Updating Matrices Time 122.892644167\n",
      "Centroid 28 : Taking candidates, Evaluating ED, Max ED: 34.640588517 , Updating Matrices Time 147.491624117\n",
      "Centroid 29 : Taking candidates, Evaluating ED, Max ED: 29.6834555103 , Updating Matrices Time 127.89989996\n",
      "Centroid 30 : Taking candidates, Evaluating ED, Max ED: 27.7118323485 , Updating Matrices Time 120.501396894\n",
      "Centroid 31 : Taking candidates, Evaluating ED, Max ED: 34.4653868447 , Updating Matrices Time 132.036782026\n",
      "Centroid 32 : Taking candidates, Evaluating ED, Max ED: 31.9414552456 , Updating Matrices Time 128.826186895\n",
      "Centroid 33 : Taking candidates, Evaluating ED, Max ED: 27.8355681146 , Updating Matrices Time 130.866215944\n",
      "Centroid 34 : Taking candidates, Evaluating ED, Max ED: 27.1575464024 , Updating Matrices Time 148.392867088\n",
      "Centroid 35 : Taking candidates, Evaluating ED, Max ED: 23.7493919616 , Updating Matrices Time 139.208637953\n",
      "Centroid 36 : Taking candidates, Evaluating ED, Max ED: 56.673181881 , Updating Matrices Time 135.905214071\n",
      "Centroid 37 : Taking candidates, Evaluating ED, Max ED: 27.5326971562 , Updating Matrices Time 145.783961058\n",
      "Centroid 38 : Taking candidates, Evaluating ED, Max ED: 102.417711099 , Updating Matrices Time 174.512181044\n",
      "Centroid 39 : Taking candidates, Evaluating ED, Max ED: 22.1991086713 , Updating Matrices Time 152.12878108\n",
      "Centroid 40 : Taking candidates, Evaluating ED, Max ED: 41.4310029315 , Updating Matrices Time 152.662354946\n",
      "Centroid 41 : Taking candidates, Evaluating ED, Max ED: 20.3559408422 , Updating Matrices Time 144.093224049\n",
      "Centroid 42 : Taking candidates, Evaluating ED, Max ED: 39.9413424459 , Updating Matrices Time 159.841302156\n",
      "Centroid 43 : Taking candidates, Evaluating ED, Max ED: 25.9913905169 , Updating Matrices Time 164.370182037\n",
      "Centroid 44 : Taking candidates, Evaluating ED, Max ED: 30.6300028957 , Updating Matrices Time 143.499798059\n",
      "Centroid 45 : Taking candidates, Evaluating ED, Max ED: 23.0607531338 , Updating Matrices Time 148.399123192\n",
      "Centroid 46 : Taking candidates, Evaluating ED, Max ED: 15.3613302219 , Updating Matrices Time 158.90464592\n",
      "Centroid 47 : Taking candidates, Evaluating ED, Max ED: 16.4814839956 , Updating Matrices Time 145.33531189\n",
      "Centroid 48 : Taking candidates, Evaluating ED, Max ED: 19.9191430577 , Updating Matrices Time 180.237633944\n",
      "Centroid 49 : Taking candidates, Evaluating ED, Max ED: 49.5893941517 , Updating Matrices Time 155.734977961\n",
      "Centroid 50 : Taking candidates, Evaluating ED, Max ED: 42.0256248677 , Updating Matrices Time 172.195767164\n",
      "Iteration 1 : DeltaW/W inf , Iteration Time 204.000916958\n",
      "Iteration 2 : DeltaW/W 0.18731245711 , Iteration Time 175.346045971\n",
      "Iteration 3 : DeltaW/W 0.149137696179 , Iteration Time 211.039562941\n",
      "Iteration 4 : DeltaW/W 0.090833956858 , Iteration Time 197.902750969\n",
      "Iteration 5 : DeltaW/W 0.0949503664722 , Iteration Time 153.273055077\n",
      "Iteration 6 : DeltaW/W 0.0715193852313 , Iteration Time 160.132463932\n",
      "Iteration 7 : DeltaW/W 0.0729505131469 , Iteration Time 175.114132166\n",
      "Iteration 8 : DeltaW/W 0.069449033008 , Iteration Time 209.172793865\n",
      "Iteration 9 : DeltaW/W 0.0569036393188 , Iteration Time 202.506606102\n",
      "Iteration 10 : DeltaW/W 0.0646455862583 , Iteration Time 181.289365053\n",
      "Iteration 11 : DeltaW/W 0.057309858314 , Iteration Time 142.044701099\n",
      "Iteration 12 : DeltaW/W 0.063193532489 , Iteration Time 160.528748989\n",
      "Iteration 13 : DeltaW/W 0.0672122054389 , Iteration Time 174.158365011\n",
      "AUCtr = 0.849576, AUCval = 0.849252, AUCtst = 0.849743\n",
      "Elapsed minutes = 151.364336\n"
     ]
    }
   ],
   "source": [
    "# definir variable de usuario y ejecutar condicionalmente cualquier código que dependa del contexto de ejecución\n",
    "# no borrar código, simplemente ejecutarlo o no con un \"if\"\n",
    "# cada uno mantiene actualizada su parte del \"if\" y no toca la del otro\n",
    "\n",
    "user = 'navia'\n",
    "#user = 'roberto'\n",
    "\n",
    "#modelo = 'hybrid' \n",
    "#modelo = 'kernelgrad' \n",
    "overwrite_results = True # overwrites results even when the result file exists, skips the execution otherwise\n",
    "\n",
    "if user == 'roberto':\n",
    "    # definir sc\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "    from pyspark import SparkConf, SparkContext\n",
    "    conf = (SparkConf().setMaster(\"local[4]\").setAppName(\"My app\").set(\"spark.executor.memory\", \"2g\"))\n",
    "    sc = SparkContext(conf = conf)\n",
    "    import common.lib.svm_utils as SVM_UTILS\n",
    "    import numpy as np\n",
    "    from pyspark.mllib.regression import LabeledPoint\n",
    "    import pickle\n",
    "    from math import sqrt\n",
    "    %matplotlib inline\n",
    "    # Se importan las funciones\n",
    "    from pyspark.mllib.util import MLUtils\n",
    "    from common.lib.IRWLSUtils import *\n",
    "    Npartitions = 12\n",
    "    Samplefraction = 0.05\n",
    "\n",
    "if user == 'navia':\n",
    "    sc.addPyFile(\"file:///export/usuarios01/navia/spark/SVM_spark/common/lib/svm_utils.py\")\n",
    "    sc.addPyFile(\"file:///export/usuarios01/navia/spark/SVM_spark/common/lib/IRWLSUtils.py\")   \n",
    "    sc.addPyFile(\"file:///export/usuarios01/navia/spark/SVM_spark/common/lib/KernelUtils.py\")   \n",
    "    sc.addPyFile(\"file:///export/usuarios01/navia/spark/SVM_spark/common/lib/ResultsUtils.py\")   \n",
    "    sc.addPyFile(\"file:///export/usuarios01/navia/spark/SVM_spark/common/lib/SGMAUtils.py\")   \n",
    "    import svm_utils as SVM_UTILS\n",
    "    #from SGMAUtils import SGMA\n",
    "    from IRWLSUtils import loadFile, train_SGMA_IRWLS, train_random_IRWLS, train_hybrid_SGMA_IRWLS\n",
    "    #from ResultsUtils import compute_AUCs\n",
    "    import numpy as np\n",
    "    from pyspark.mllib.regression import LabeledPoint\n",
    "    from pyspark.mllib.linalg import SparseVector, DenseVector\n",
    "    import pickle\n",
    "    from math import sqrt\n",
    "    from pyspark.mllib.util import MLUtils\n",
    "    %matplotlib inline\n",
    "    Npartitions = 32\n",
    "    Samplefraction = 0.05\n",
    "\n",
    "# 0 = Ripley\n",
    "# 1 = Kwok\n",
    "# 2 = Twonorm\n",
    "# 3 = Waveform\n",
    "# 4 = Adult\n",
    "# 5 = SUSY\n",
    "# 6 = KddCup1999\n",
    "\n",
    "datasets = [0, 1, 2, 3, 4, 5, 6]\n",
    "folds = [0, 1, 2, 3, 4]\n",
    "dataset_names = ['Ripley', 'Kwok', 'Twonorm', 'Waveform', 'Adult', 'Susy', 'KddCup1999']\n",
    "Niters = [50, 100, 200]\n",
    "NCs = [5, 10, 25, 50, 100, 200]\n",
    "modelos = ['hybridgrad', 'kernelgrad', 'SGMA_IRWLS', 'LinearSVM', 'Logistic', 'random_IRWLS', 'hybrid_IRWLS','SGMA_IRWLS']\n",
    "\n",
    "datasets = [5]\n",
    "folds = [0]\n",
    "modelos = ['hybridgrad']\n",
    "Niters = [150]\n",
    "NCs = [20]\n",
    "Samplefraction = 0.05\n",
    "\n",
    "for modelo in modelos:\n",
    "    for kdataset in datasets:\n",
    "        for kfold in folds:\n",
    "            for Niter in Niters:\n",
    "                C = 100.0\n",
    "                name_dataset = dataset_names[kdataset]\n",
    "                \n",
    "                #####################################################################################################################\n",
    "                # DATA LOADING: the result of this part must always be three RDDs for train, val and test, containing labelled points.\n",
    "                #####################################################################################################################\n",
    "                if kdataset in [0, 1, 2, 3]: # loading from .mat\n",
    "                    x_tr, y_tr, x_val, y_val, x_tst, y_tst = SVM_UTILS.load_data(kdataset, kfold)\n",
    "                    NI = x_tr.shape[1]\n",
    "                    sigma = sqrt(NI)\n",
    "                    \n",
    "                    XtrRDD = sc.parallelize(np.hstack((y_tr, x_tr)), Npartitions).map(lambda x: LabeledPoint(x[0], x[1:]))\n",
    "                    XvalRDD = sc.parallelize(np.hstack((y_val, x_val)), Npartitions).map(lambda x: LabeledPoint(x[0], x[1:]))\n",
    "                    XtstRDD = sc.parallelize(np.hstack((y_tst, x_tst)), Npartitions).map(lambda x: LabeledPoint(x[0], x[1:]))\n",
    "\n",
    "                if kdataset in [4]: # loading libsvm format\n",
    "                    if kdataset == 4:\n",
    "                        dimensions = 123                       \n",
    "                    print \"Loading \" + name_dataset\n",
    "                    XtrRDD = loadFile('./data/' + name_dataset.lower() + '_train',sc,dimensions,Npartitions)\n",
    "                    XvalRDD = loadFile('./data/' + name_dataset.lower() + '_val',sc,dimensions,Npartitions)\n",
    "                    XtstRDD = loadFile('./data/' + name_dataset.lower() + '_test',sc,dimensions,Npartitions)\n",
    "                    sigma = np.sqrt(dimensions)\n",
    "                    \n",
    "                if kdataset in [5]: # loading in libsvm format and splitting RDD\n",
    "                    if kdataset == 5:\n",
    "                        filename = 'file:///export/usuarios01/navia/spark/SVM_spark/data/SUSY.txt'\n",
    "                        #filename = 'file:///export/usuarios01/navia/spark/SVM_spark/data/minisusy.txt'\n",
    "                        dimensions = 18\n",
    "                    rawdata = MLUtils.loadLibSVMFile(sc, filename)\n",
    "                    # Labelled points are sparse, we map the values\n",
    "                    rawdata = rawdata.map(lambda x: LabeledPoint(x.label, DenseVector((x.features).toArray())  ))\n",
    "                    XtrRDD, XvalRDD, XtstRDD = rawdata.randomSplit(weights=[0.7, 0.1, 0.2], seed=1234)\n",
    "                    sigma = np.sqrt(dimensions)\n",
    "                \n",
    "                #XtrRDD.cache()\n",
    "                #XvalRDD.cache()\n",
    "                #XtstRDD.cache()\n",
    "                #################################   END LOADING DATA ##########################################################\n",
    "                \n",
    "                for NC in NCs:\n",
    "                    print \"Dataset = %s, modelo = %s, kfold = %d, Niter = %d, NC = %d\" % (name_dataset, modelo, kfold, Niter, NC)\n",
    "                    filename = './results/dataset_' + str(kdataset) + '_modelo_' + modelo + '_NC_' + str(NC) + '_Niter_' + str(Niter) + '_kfold_' + str(kfold) + '.pkl'\n",
    "                    #import code\n",
    "                    #code.interact(local=locals())\n",
    "                    try:\n",
    "                        f = open(filename,'r')\n",
    "                        f.close()\n",
    "                        file_exists = True\n",
    "                    except:\n",
    "                        file_exists = False\n",
    "                        pass\n",
    "                    execute = False\n",
    "                    if file_exists:\n",
    "                        if overwrite_results:\n",
    "                            execute = True\n",
    "                    else:\n",
    "                        execute = True                                  \n",
    "                    if execute:\n",
    "\n",
    "                        if modelo == 'hybridgrad':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time = SVM_UTILS.train_hybridSVM(XtrRDD, XvalRDD, XtstRDD, sigma, C, NC, Niter, Samplefraction)\n",
    "\n",
    "                        if modelo == 'kernelgrad':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time = SVM_UTILS.train_kernelgrad(XtrRDD, XvalRDD, XtstRDD, sigma, C, NC, Niter, Samplefraction)\n",
    "\n",
    "                        if modelo == 'SGMA_IRWLS':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time = train_SGMA_IRWLS(XtrRDD, XvalRDD, XtstRDD, sigma, C, NC, Niter)\n",
    "\n",
    "                        if modelo == 'LinearSVM':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time = SVM_UTILS.train_linear_SVM(XtrRDD, XvalRDD, XtstRDD)\n",
    "                                                \n",
    "                        if modelo == 'Logistic':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time = SVM_UTILS.train_logistic(XtrRDD, XvalRDD, XtstRDD)\n",
    "\n",
    "                        if modelo == 'random_IRWLS':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time = train_random_IRWLS(XtrRDD, XvalRDD, XtstRDD, sigma, C, NC, Niter)\n",
    "\n",
    "                        if modelo == 'hybrid_IRWLS':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time = train_hybrid_SGMA_IRWLS(XtrRDD, XvalRDD, XtstRDD, sigma, C, NC, Niter)\n",
    "\n",
    "                        print \"AUCtr = %f, AUCval = %f, AUCtst = %f\" % (auc_tr, auc_val, auc_tst)\n",
    "                        print \"Elapsed minutes = %f\" % (exe_time / 60.0)\n",
    "\n",
    "                        with open(filename, 'w') as f:\n",
    "                            pickle.dump([auc_tr, auc_val, auc_tst, exe_time], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
