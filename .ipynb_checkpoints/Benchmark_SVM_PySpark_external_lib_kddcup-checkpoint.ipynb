{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Semiparametric SVM training using subgradients in Spark **\n",
    "\n",
    "#### bla, bla, bla. \n",
    "\n",
    "#### We will benchmark the algorithms with data files from UCI:\n",
    "\n",
    "* **Ripley**, the Ripley dataset\n",
    "* **Kwok**, the Kwok dataset\n",
    "* **Twonorm**, the Twonorm dataset\n",
    "* **Waveform**, the Waveform dataset\n",
    "* **Covertype**, the Covertype dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset = KddCup1999, modelo = SGMA_IRWLS, kfold = 0, Niter = 300, NC = 150\n",
      "Centroid 1 : Taking candidates, Evaluating ED, Max ED: 745.437381654 , Updating Matrices Time 3.26134705544\n",
      "Centroid 2 : Taking candidates, Evaluating ED, Max ED: 54.0561237054 , Updating Matrices Time 8.55704689026\n",
      "Centroid 3 : Taking candidates, Evaluating ED, Max ED: 6.14281597804 , Updating Matrices Time 6.77985596657\n",
      "Centroid 4 : Taking candidates, Evaluating ED, Max ED: 1.58848330561 , Updating Matrices Time 13.816999197\n",
      "Centroid 5 : Taking candidates, Evaluating ED, Max ED: 0.173178883879 , Updating Matrices Time 9.95810294151\n",
      "Centroid 6 : Taking candidates, Evaluating ED, Max ED: 4.33614301695 , Updating Matrices Time 8.29969501495\n",
      "Centroid 7 : Taking candidates, Evaluating ED, Max ED: 0.0966355591771 , Updating Matrices Time 9.46906995773\n",
      "Centroid 8 : Taking candidates, Evaluating ED, Max ED: 0.242561240929 , Updating Matrices Time 9.24091696739\n",
      "Centroid 9 : Taking candidates, Evaluating ED, Max ED: 0.776180734998 , Updating Matrices Time 5.74257183075\n",
      "Centroid 10 : Taking candidates, Evaluating ED, Max ED: 0.16349325889 , Updating Matrices Time 5.722645998\n",
      "Centroid 11 : Taking candidates, Evaluating ED, Max ED: 0.803981401522 , Updating Matrices Time 6.15403223038\n",
      "Centroid 12 : Taking candidates, Evaluating ED, Max ED: 0.025839445706 , Updating Matrices Time 5.65735197067\n",
      "Centroid 13 : Taking candidates, Evaluating ED, Max ED: 0.0173873436887 , Updating Matrices Time 6.04528689384\n",
      "Centroid 14 : Taking candidates, Evaluating ED, Max ED: 0.0329192271829 , Updating Matrices Time 5.6945950985\n",
      "Centroid 15 : Taking candidates, Evaluating ED, Max ED: 0.0286802594986 , Updating Matrices Time 5.69001197815\n",
      "Centroid 16 : Taking candidates, Evaluating ED, Max ED: 0.451131056829 , Updating Matrices Time 5.72641706467\n",
      "Centroid 17 : Taking candidates, Evaluating ED, Max ED: 2.02919690159 , Updating Matrices Time 5.875467062\n",
      "Centroid 18 : Taking candidates, Evaluating ED, Max ED: 0.088445190851 , Updating Matrices Time 5.97269701958\n",
      "Centroid 19 : Taking candidates, Evaluating ED, Max ED: 0.00494529551303 , Updating Matrices Time 6.78679609299\n",
      "Centroid 20 : Taking candidates, Evaluating ED, Max ED: 0.00659435701694 , Updating Matrices Time 5.94506692886\n",
      "Centroid 21 : Taking candidates, Evaluating ED, Max ED: 0.152100606278 , Updating Matrices Time 5.71552085876\n",
      "Centroid 22 : Taking candidates, Evaluating ED, Max ED: 0.492525142427 , Updating Matrices Time 5.97224903107\n",
      "Centroid 23 : Taking candidates, Evaluating ED, Max ED: 0.0184875755109 , Updating Matrices Time 5.85558104515\n",
      "Centroid 24 : Taking candidates, Evaluating ED, Max ED: 0.00538290517018 , Updating Matrices Time 6.19116401672\n",
      "Centroid 25 : Taking candidates, Evaluating ED, Max ED: 0.00600172733517 , Updating Matrices Time 6.30959701538\n",
      "Centroid 26 : Taking candidates, Evaluating ED, Max ED: 0.000915071970975 , Updating Matrices Time 5.93715906143\n",
      "Centroid 27 : Taking candidates, Evaluating ED, Max ED: 0.00123120625468 , Updating Matrices Time 5.88676190376\n",
      "Centroid 28 : Taking candidates, Evaluating ED, Max ED: 0.00132020580452 , Updating Matrices Time 5.86694788933\n",
      "Centroid 29 : Taking candidates, Evaluating ED, Max ED: 0.000454506341998 , Updating Matrices Time 6.15943598747\n",
      "Centroid 30 : Taking candidates, Evaluating ED, Max ED: 0.0153543217907 , Updating Matrices Time 6.0135178566\n",
      "Centroid 31 : Taking candidates, Evaluating ED, Max ED: 0.000764357034771 , Updating Matrices Time 6.51712989807\n",
      "Centroid 32 : Taking candidates, Evaluating ED, Max ED: 0.00084951676895 , Updating Matrices Time 6.39408016205\n",
      "Centroid 33 : Taking candidates, Evaluating ED, Max ED: 0.00337704524986 , Updating Matrices Time 6.31140017509\n",
      "Centroid 34 : Taking candidates, Evaluating ED, Max ED: 0.000738164547168 , Updating Matrices Time 6.54922008514\n",
      "Centroid 35 : Taking candidates, Evaluating ED, Max ED: 0.0128664125579 , Updating Matrices Time 6.02252817154\n",
      "Centroid 36 : Taking candidates, Evaluating ED, Max ED: 0.000989244851338 , Updating Matrices Time 6.60521411896\n",
      "Centroid 37 : Taking candidates, Evaluating ED, Max ED: 0.000605896884137 , Updating Matrices Time 6.24876594543\n",
      "Centroid 38 : Taking candidates, Evaluating ED, Max ED: 0.0206138297772 , Updating Matrices Time 6.86005091667\n",
      "Centroid 39 : Taking candidates, Evaluating ED, Max ED: 0.000171186650447 , Updating Matrices Time 6.38281989098\n",
      "Centroid 40 : Taking candidates, Evaluating ED, Max ED: 0.000191668401534 , Updating Matrices Time 6.10732698441\n",
      "Centroid 41 : Taking candidates, Evaluating ED, Max ED: 0.000825793605241 , Updating Matrices Time 6.4755179882\n",
      "Centroid 42 : Taking candidates, Evaluating ED, Max ED: 5.49247351335e-05 , Updating Matrices Time 6.42297792435\n",
      "Centroid 43 : Taking candidates, Evaluating ED, Max ED: 0.0254481315391 , Updating Matrices Time 7.16774106026\n",
      "Centroid 44 : Taking candidates, Evaluating ED, Max ED: 0.000583725783843 , Updating Matrices Time 6.12820100784\n",
      "Centroid 45 : Taking candidates, Evaluating ED, Max ED: 0.000475248384793 , Updating Matrices Time 7.95242595673\n",
      "Centroid 46 : Taking candidates, Evaluating ED, Max ED: 0.000210682138346 , Updating Matrices Time 6.4493470192\n",
      "Centroid 47 : Taking candidates, Evaluating ED, Max ED: 3.59609901959e-05 , Updating Matrices Time 6.01923584938\n",
      "Centroid 48 : Taking candidates, Evaluating ED, Max ED: 0.00377294180566 , Updating Matrices Time 5.84572410583\n",
      "Centroid 49 : Taking candidates, Evaluating ED, Max ED: 0.00111851054436 , Updating Matrices Time 6.98442983627\n",
      "Centroid 50 : Taking candidates, Evaluating ED, Max ED: 0.00514987009905 , Updating Matrices Time 9.17281198502\n",
      "Centroid 51 : Taking candidates, Evaluating ED, Max ED: 6.0497824288e-05 , Updating Matrices Time 7.79940700531\n",
      "Centroid 52 : Taking candidates, Evaluating ED, Max ED: 0.0106010557546 , Updating Matrices Time 10.3221139908\n",
      "Centroid 53 : Taking candidates, Evaluating ED, Max ED: 8.26882216853e-05 , Updating Matrices Time 9.67870998383\n",
      "Centroid 54 : Taking candidates, Evaluating ED, Max ED: 6.478315955e-05 , Updating Matrices Time 10.1508648396\n",
      "Centroid 55 : Taking candidates, Evaluating ED, Max ED: 4.67500047893e-05 , Updating Matrices Time 11.4673531055\n",
      "Centroid 56 : Taking candidates, Evaluating ED, Max ED: 0.00755346587146 , Updating Matrices Time 17.5817768574\n",
      "Centroid 57 : Taking candidates, Evaluating ED, Max ED: 0.000443721776157 , Updating Matrices Time 18.0850379467\n",
      "Centroid 58 : Taking candidates, Evaluating ED, Max ED: 2.94252013717e-05 , Updating Matrices Time 13.6021809578\n",
      "Centroid 59 : Taking candidates, Evaluating ED, Max ED: 4.60078127251e-05 , Updating Matrices Time 16.1857659817\n",
      "Centroid 60 : Taking candidates, Evaluating ED, Max ED: 0.000118718444898 , Updating Matrices Time 16.2916510105\n",
      "Centroid 61 : Taking candidates, Evaluating ED, Max ED: 0.0417652390887 , Updating Matrices Time 16.663132906\n",
      "Centroid 62 : Taking candidates, Evaluating ED, Max ED: 0.00207231674487 , Updating Matrices Time 19.9989030361\n",
      "Centroid 63 : Taking candidates, Evaluating ED, Max ED: 2.64999923811e-05 , Updating Matrices Time 19.5168480873\n",
      "Centroid 64 : Taking candidates, Evaluating ED, Max ED: 2.59628438896e-05 , Updating Matrices Time 14.0101702213\n",
      "Centroid 65 : Taking candidates, Evaluating ED, Max ED: 0.00108647239898 , Updating Matrices Time 15.6191811562\n",
      "Centroid 66 : Taking candidates, Evaluating ED, Max ED: 4.36220774293e-05 , Updating Matrices Time 20.2117500305\n",
      "Centroid 67 : Taking candidates, Evaluating ED, Max ED: 0.000363538216072 , Updating Matrices Time 15.9443118572\n",
      "Centroid 68 : Taking candidates, Evaluating ED, Max ED: 1.64137249917e-05 , Updating Matrices Time 13.6567728519\n",
      "Centroid 69 : Taking candidates, Evaluating ED, Max ED: 0.000296740870258 , Updating Matrices Time 18.0274600983\n",
      "Centroid 70 : Taking candidates, Evaluating ED, Max ED: 2.96188914462e-05 , Updating Matrices Time 17.5904071331\n",
      "Centroid 71 : Taking candidates, Evaluating ED, Max ED: 0.0001132684105 , Updating Matrices Time 16.5603079796\n",
      "Centroid 72 : Taking candidates, Evaluating ED, Max ED: 1.44594982155e-05 , Updating Matrices Time 19.2941410542\n",
      "Centroid 73 : Taking candidates, Evaluating ED, Max ED: 0.00674230818658 , Updating Matrices Time 19.3422150612\n",
      "Centroid 74 : Taking candidates, Evaluating ED, Max ED: 2.7967491237e-06 , Updating Matrices Time 18.6461720467\n",
      "Centroid 75 : Taking candidates, Evaluating ED, Max ED: 0.000115985227082 , Updating Matrices Time 24.6325790882\n",
      "Centroid 76 : Taking candidates, Evaluating ED, Max ED: 0.000226262721056 , Updating Matrices Time 17.6627440453\n",
      "Centroid 77 : Taking candidates, Evaluating ED, Max ED: 3.88191862276e-06 , Updating Matrices Time 18.7273740768\n",
      "Centroid 78 : Taking candidates, Evaluating ED, Max ED: 0.00110078598134 , Updating Matrices Time 19.6795380116\n",
      "Centroid 79 : Taking candidates, Evaluating ED, Max ED: 1.69349354931e-05 , Updating Matrices Time 16.8247208595\n",
      "Centroid 80 : Taking candidates, Evaluating ED, Max ED: 8.08770003367e-06 , Updating Matrices Time 24.5428259373\n",
      "Centroid 81 : Taking candidates, Evaluating ED, Max ED: 2.03499289054e-05 , Updating Matrices Time 22.3461580276\n",
      "Centroid 82 : Taking candidates, Evaluating ED, Max ED: 2.64525846605e-05 , Updating Matrices Time 17.9429030418\n",
      "Centroid 83 : Taking candidates, Evaluating ED, Max ED: 9.9205193018e-05 , Updating Matrices Time 15.5267701149\n",
      "Centroid 84 : Taking candidates, Evaluating ED, Max ED: 2.85849424434e-05 , Updating Matrices Time 18.1086211205\n",
      "Centroid 85 : Taking candidates, Evaluating ED, Max ED: 5.07654553391e-05 , Updating Matrices Time 19.1339111328\n",
      "Centroid 86 : Taking candidates, Evaluating ED, Max ED: 2.71042695317e-06 , Updating Matrices Time 17.7565450668\n",
      "Centroid 87 : Taking candidates, Evaluating ED, Max ED: 1.4140225741e-06 , Updating Matrices Time 21.5086538792\n",
      "Centroid 88 : Taking candidates, Evaluating ED, Max ED: 6.87871547673e-07 , Updating Matrices Time 19.5091779232\n",
      "Centroid 89 : Taking candidates, Evaluating ED, Max ED: 0.00446366121521 , Updating Matrices Time 19.9272019863\n",
      "Centroid 90 : Taking candidates, Evaluating ED, Max ED: 0.000612774390549 , Updating Matrices Time 24.1114768982\n",
      "Centroid 91 : Taking candidates, Evaluating ED, Max ED: 7.92006593137e-07 , Updating Matrices Time 13.0930120945\n",
      "Centroid 92 : Taking candidates, Evaluating ED, Max ED: 2.27993482978e-06 , Updating Matrices Time 13.1342060566\n",
      "Centroid 93 : Taking candidates, Evaluating ED, Max ED: 2.56378069811e-05 , Updating Matrices Time 12.7827339172\n",
      "Centroid 94 : Taking candidates, Evaluating ED, Max ED: 2.82812405338e-07 , Updating Matrices Time 17.0588400364\n",
      "Centroid 95 : Taking candidates, Evaluating ED, Max ED: 6.28526868285e-07 , Updating Matrices Time 12.9119980335\n",
      "Centroid 96 : Taking candidates, Evaluating ED, Max ED: 0.000165660707371 , Updating Matrices Time 14.0803539753\n",
      "Centroid 97 : Taking candidates, Evaluating ED, Max ED: 3.10278675437e-07 , Updating Matrices Time 16.0377120972\n",
      "Centroid 98 : Taking candidates, Evaluating ED, Max ED: 5.3888149418e-06 , Updating Matrices Time 15.7560489178\n",
      "Centroid 99 : Taking candidates, Evaluating ED, Max ED: 1.95197612613e-06 , Updating Matrices Time 15.4963710308\n",
      "Centroid 100 : Taking candidates, Evaluating ED, Max ED: 0.0615058696822 , Updating Matrices Time 14.9247550964\n",
      "Centroid 101 : Taking candidates, Evaluating ED, Max ED: 6.15541431951e-07 , Updating Matrices Time 15.8732991219\n",
      "Centroid 102 : Taking candidates, Evaluating ED, Max ED: 1.86387543656e-06 , Updating Matrices Time 15.1121151447\n",
      "Centroid 103 : Taking candidates, Evaluating ED, Max ED: 0.000128462797303 , Updating Matrices Time 14.4891839027\n",
      "Centroid 104 : Taking candidates, Evaluating ED, Max ED: 1.74739828005e-07 , Updating Matrices Time 13.9829769135\n",
      "Centroid 105 : Taking candidates, Evaluating ED, Max ED: 2.01542283849e-05 , Updating Matrices Time 13.2633340359\n",
      "Centroid 106 : Taking candidates, Evaluating ED, Max ED: 0.000133553079794 , Updating Matrices Time 19.7332980633\n",
      "Centroid 107 : Taking candidates, Evaluating ED, Max ED: 1.62871166188e-06 , Updating Matrices Time 12.439250946\n",
      "Centroid 108 : Taking candidates, Evaluating ED, Max ED: 6.25310497134e-09 , Updating Matrices Time 15.0739440918\n",
      "Centroid 109 : Taking candidates, Evaluating ED, Max ED: 5.77332935552e-06 , Updating Matrices Time 18.1263020039\n",
      "Centroid 110 : Taking candidates, Evaluating ED, Max ED: 7.89336853898e-08 , Updating Matrices Time 16.145195961\n",
      "Centroid 111 : Taking candidates, Evaluating ED, Max ED: 0.00028108538991 , Updating Matrices Time 17.9880270958\n",
      "Centroid 112 : Taking candidates, Evaluating ED, Max ED: 7.09660180075e-07 , Updating Matrices Time 16.3986699581\n",
      "Centroid 113 : Taking candidates, Evaluating ED, Max ED: 7.76541934169e-08 , Updating Matrices Time 18.0175821781\n",
      "Centroid 114 : Taking candidates, Evaluating ED, Max ED: 0.000168558415756 , Updating Matrices Time 17.7350161076\n",
      "Centroid 115 : Taking candidates, Evaluating ED, Max ED: 0.00162518680037 , Updating Matrices Time 17.8494980335\n",
      "Centroid 116 : Taking candidates, Evaluating ED, Max ED: 5.83151646753e-09 , Updating Matrices Time 14.285451889\n",
      "Centroid 117 : Taking candidates, Evaluating ED, Max ED: 4.17433787792e-06 , Updating Matrices Time 15.0741231441\n",
      "Centroid 118 : Taking candidates, Evaluating ED, Max ED: 1.91240686601e-08 , Updating Matrices Time 22.2692470551\n",
      "Centroid 119 : Taking candidates, Evaluating ED, Max ED: 6.1057933742e-06 , Updating Matrices Time 16.6874201298\n",
      "Centroid 120 : Taking candidates, Evaluating ED, Max ED: 4.40413549588e-06 , Updating Matrices Time 18.9380180836\n",
      "Centroid 121 : Taking candidates, Evaluating ED, Max ED: 1.60116218729e-09 , Updating Matrices Time 16.9865889549\n",
      "Centroid 122 : Taking candidates, Evaluating ED, Max ED: 2.69101898063e-08 , Updating Matrices Time 17.3953838348\n",
      "Centroid 123 : Taking candidates, Evaluating ED, Max ED: 2.57316352976e-06 , Updating Matrices Time 17.5594801903\n",
      "Centroid 124 : Taking candidates, Evaluating ED, Max ED: 2.3556349644e-05 , Updating Matrices Time 21.4330010414\n",
      "Centroid 125 : Taking candidates, Evaluating ED, Max ED: 1.17338963318e-05 , Updating Matrices Time 15.3355600834\n",
      "Centroid 126 : Taking candidates, Evaluating ED, Max ED: 7.33834689157e-06 , Updating Matrices Time 20.6852061749\n",
      "Centroid 127 : Taking candidates, Evaluating ED, Max ED: 1.5325707117e-09 , Updating Matrices Time 20.1886651516\n",
      "Centroid 128 : Taking candidates, Evaluating ED, Max ED: 7.14029837873e-07 , Updating Matrices Time 17.7306251526\n",
      "Centroid 129 : Taking candidates, Evaluating ED,"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 334 in stage 1482.0 failed 4 times, most recent failure: Lost task 334.3 in stage 1482.0 (TID 358957, node84.cluster.tsc.uc3m.es): org.apache.spark.storage.BlockFetchException: Failed to fetch block from 1 locations. Most recent failure cause:\n\tat org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:595)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:585)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.storage.BlockManager.doGetRemote(BlockManager.scala:585)\n\tat org.apache.spark.storage.BlockManager.getRemote(BlockManager.scala:570)\n\tat org.apache.spark.storage.BlockManager.get(BlockManager.scala:630)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Failed to connect to node27.cluster.tsc.uc3m.es/10.0.13.47:46502\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\t... 3 more\nCaused by: java.net.ConnectException: Connection refused: node27.cluster.tsc.uc3m.es/10.0.13.47:46502\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.storage.BlockFetchException: Failed to fetch block from 1 locations. Most recent failure cause:\n\tat org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:595)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:585)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.storage.BlockManager.doGetRemote(BlockManager.scala:585)\n\tat org.apache.spark.storage.BlockManager.getRemote(BlockManager.scala:570)\n\tat org.apache.spark.storage.BlockManager.get(BlockManager.scala:630)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: java.io.IOException: Failed to connect to node27.cluster.tsc.uc3m.es/10.0.13.47:46502\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\t... 3 more\nCaused by: java.net.ConnectException: Connection refused: node27.cluster.tsc.uc3m.es/10.0.13.47:46502\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-bb97f845ab86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mmodelo\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'SGMA_IRWLS'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m                             \u001b[0mauc_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauc_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauc_tst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexe_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_SGMA_IRWLS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrRDD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXvalRDD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtstRDD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNiter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mmodelo\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'LinearSVM'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/export/workdir/spark/tmp/spark-783849bd-0464-4752-98fb-48fb1c05a02e/userFiles-ba20d43e-7d0b-4476-80c0-9a54942072d9/IRWLSUtils.py\u001b[0m in \u001b[0;36mtrain_SGMA_IRWLS\u001b[1;34m(XtrRDD, XvalRDD, XtstRDD, sigma, C, NC, Niter, stop_criteria)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[0mtime_ini\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m     \u001b[0mBases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSGMA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrRDD\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msamplingRate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[0mPesos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIRWLS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrRDD\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBases\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_criteria\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop_criteria\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/export/workdir/spark/tmp/spark-783849bd-0464-4752-98fb-48fb1c05a02e/userFiles-ba20d43e-7d0b-4476-80c0-9a54942072d9/SGMAUtils.py\u001b[0m in \u001b[0;36mSGMA\u001b[1;34m(originaldataset, numCentros, sigma, samplingRate)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;31m# Cluster variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mDescenso\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0m_errorDecrease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mzetalist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msamplingRate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metalist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark-1.6.0-bin-2.6.0/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    795\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 797\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    798\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark-1.6.0-bin-2.6.0/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    769\u001b[0m         \"\"\"\n\u001b[0;32m    770\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark-1.6.0-bin-2.6.0/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark-1.6.0-bin-2.6.0/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark-1.6.0-bin-2.6.0/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 334 in stage 1482.0 failed 4 times, most recent failure: Lost task 334.3 in stage 1482.0 (TID 358957, node84.cluster.tsc.uc3m.es): org.apache.spark.storage.BlockFetchException: Failed to fetch block from 1 locations. Most recent failure cause:\n\tat org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:595)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:585)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.storage.BlockManager.doGetRemote(BlockManager.scala:585)\n\tat org.apache.spark.storage.BlockManager.getRemote(BlockManager.scala:570)\n\tat org.apache.spark.storage.BlockManager.get(BlockManager.scala:630)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Failed to connect to node27.cluster.tsc.uc3m.es/10.0.13.47:46502\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\t... 3 more\nCaused by: java.net.ConnectException: Connection refused: node27.cluster.tsc.uc3m.es/10.0.13.47:46502\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.storage.BlockFetchException: Failed to fetch block from 1 locations. Most recent failure cause:\n\tat org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:595)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:585)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.storage.BlockManager.doGetRemote(BlockManager.scala:585)\n\tat org.apache.spark.storage.BlockManager.getRemote(BlockManager.scala:570)\n\tat org.apache.spark.storage.BlockManager.get(BlockManager.scala:630)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: java.io.IOException: Failed to connect to node27.cluster.tsc.uc3m.es/10.0.13.47:46502\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\t... 3 more\nCaused by: java.net.ConnectException: Connection refused: node27.cluster.tsc.uc3m.es/10.0.13.47:46502\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# definir variable de usuario y ejecutar condicionalmente cualquier cdigo que dependa del contexto de ejecucin\n",
    "# no borrar cdigo, simplemente ejecutarlo o no con un \"if\"\n",
    "# cada uno mantiene actualizada su parte del \"if\" y no toca la del otro\n",
    "\n",
    "user = 'navia'\n",
    "#user = 'roberto'\n",
    "\n",
    "#modelo = 'hybrid' \n",
    "#modelo = 'kernelgrad' \n",
    "overwrite_results = True # overwrites results even when the result file exists, skips the execution otherwise\n",
    "\n",
    "if user == 'roberto':\n",
    "    # definir sc\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "    from pyspark import SparkConf, SparkContext\n",
    "    conf = (SparkConf().setMaster(\"local[4]\").setAppName(\"My app\").set(\"spark.executor.memory\", \"2g\"))\n",
    "    sc = SparkContext(conf = conf)\n",
    "    import common.lib.svm_utils as SVM_UTILS\n",
    "    import numpy as np\n",
    "    from pyspark.mllib.regression import LabeledPoint\n",
    "    import pickle\n",
    "    from math import sqrt\n",
    "    %matplotlib inline\n",
    "    # Se importan las funciones\n",
    "    from pyspark.mllib.util import MLUtils\n",
    "    from common.lib.IRWLSUtils import *\n",
    "    Npartitions = 12\n",
    "    Samplefraction = 0.05\n",
    "\n",
    "if user == 'navia':\n",
    "    sc.addPyFile(\"file:///export/usuarios01/navia/spark/SVM_spark/common/lib/svm_utils.py\")\n",
    "    sc.addPyFile(\"file:///export/usuarios01/navia/spark/SVM_spark/common/lib/IRWLSUtils.py\")   \n",
    "    sc.addPyFile(\"file:///export/usuarios01/navia/spark/SVM_spark/common/lib/KernelUtils.py\")   \n",
    "    sc.addPyFile(\"file:///export/usuarios01/navia/spark/SVM_spark/common/lib/ResultsUtils.py\")   \n",
    "    sc.addPyFile(\"file:///export/usuarios01/navia/spark/SVM_spark/common/lib/SGMAUtils.py\")   \n",
    "    import svm_utils as SVM_UTILS\n",
    "    #from SGMAUtils import SGMA\n",
    "    from IRWLSUtils import loadFile, train_SGMA_IRWLS, train_random_IRWLS, train_hybrid_SGMA_IRWLS, train_kmeans_IRWLS\n",
    "    #from ResultsUtils import compute_AUCs\n",
    "    import numpy as np\n",
    "    from pyspark.mllib.regression import LabeledPoint\n",
    "    from pyspark.mllib.linalg import SparseVector, DenseVector\n",
    "    import pickle\n",
    "    from math import sqrt\n",
    "    from pyspark.mllib.util import MLUtils\n",
    "    %matplotlib inline\n",
    "    Npartitions = -99 # cuando es menor que 0, no se aplica y se deja libre\n",
    "    Samplefraction = 0.05\n",
    "\n",
    "# 0 = Ripley\n",
    "# 1 = Kwok\n",
    "# 2 = Twonorm\n",
    "# 3 = Waveform\n",
    "# 4 = Adult\n",
    "# 5 = SUSY\n",
    "# 6 = KddCup1999, requiere preprocesado...\n",
    "# 7 = Higgs\n",
    "\n",
    "datasets = [0, 1, 2, 3, 4, 5, 6]\n",
    "folds = [0, 1, 2, 3, 4]\n",
    "dataset_names = ['Ripley', 'Kwok', 'Twonorm', 'Waveform', 'Adult', 'Susy', 'KddCup1999', 'Higgs']\n",
    "Niters = [50, 100, 200]\n",
    "NCs = [5, 10, 25, 50, 100, 200]\n",
    "modelos = ['hybridgrad', 'kernelgrad', 'SGMA_IRWLS', 'LinearSVM', 'Logistic', 'random_IRWLS', 'hybrid_IRWLS', 'Kmeans_IRWLS']\n",
    "\n",
    "datasets = [7]\n",
    "folds = [0]\n",
    "modelos = ['SGMA_IRWLS']\n",
    "Niters = [300]\n",
    "NCs = [100]\n",
    "Samplefraction = 0.05\n",
    "fsigma = 1.0\n",
    "\n",
    "for modelo in modelos:\n",
    "    for kdataset in datasets:\n",
    "        for kfold in folds:\n",
    "            for Niter in Niters:\n",
    "                C = 100.0\n",
    "                name_dataset = dataset_names[kdataset]\n",
    "                \n",
    "                #####################################################################################################################\n",
    "                # DATA LOADING: the result of this part must always be three RDDs for train, val and test, containing labelled points.\n",
    "                #####################################################################################################################\n",
    "                if kdataset in [0, 1, 2, 3]: # loading from .mat\n",
    "                    x_tr, y_tr, x_val, y_val, x_tst, y_tst = SVM_UTILS.load_data(kdataset, kfold)\n",
    "                    NI = x_tr.shape[1]\n",
    "                    sigma = fsigma * sqrt(NI)\n",
    "                    \n",
    "                    if Npartitions > 0:\n",
    "                        XtrRDD = sc.parallelize(np.hstack((y_tr, x_tr)), Npartitions).map(lambda x: LabeledPoint(x[0], x[1:]))\n",
    "                        XvalRDD = sc.parallelize(np.hstack((y_val, x_val)), Npartitions).map(lambda x: LabeledPoint(x[0], x[1:]))\n",
    "                        XtstRDD = sc.parallelize(np.hstack((y_tst, x_tst)), Npartitions).map(lambda x: LabeledPoint(x[0], x[1:]))\n",
    "                    else:\n",
    "                        XtrRDD = sc.parallelize(np.hstack((y_tr, x_tr))).map(lambda x: LabeledPoint(x[0], x[1:]))\n",
    "                        XvalRDD = sc.parallelize(np.hstack((y_val, x_val))).map(lambda x: LabeledPoint(x[0], x[1:]))\n",
    "                        XtstRDD = sc.parallelize(np.hstack((y_tst, x_tst))).map(lambda x: LabeledPoint(x[0], x[1:]))\n",
    "                       \n",
    "                        \n",
    "                if kdataset in [4]: # loading libsvm format\n",
    "                    if kdataset == 4:\n",
    "                        dimensions = 123                       \n",
    "                    print \"Loading \" + name_dataset\n",
    "                    XtrRDD = loadFile('./data/' + name_dataset.lower() + '_train',sc,dimensions,Npartitions)\n",
    "                    XvalRDD = loadFile('./data/' + name_dataset.lower() + '_val',sc,dimensions,Npartitions)\n",
    "                    XtstRDD = loadFile('./data/' + name_dataset.lower() + '_test',sc,dimensions,Npartitions)\n",
    "                    sigma = fsigma * np.sqrt(dimensions)\n",
    "                    \n",
    "                if kdataset in [5]: # loading in libsvm format and splitting RDD\n",
    "                    if kdataset == 5:\n",
    "                        filename = 'file:///export/usuarios01/navia/spark/SVM_spark/data/SUSY.txt'\n",
    "                        #filename = 'file:///export/usuarios01/navia/spark/SVM_spark/data/minisusy.txt'\n",
    "                        dimensions = 18\n",
    "                    rawdata = MLUtils.loadLibSVMFile(sc, filename)\n",
    "                    # Labelled points are sparse, we map the values\n",
    "                    rawdata = rawdata.map(lambda x: LabeledPoint(x.label, DenseVector((x.features).toArray())  ))\n",
    "                    XtrRDD, XvalRDD, XtstRDD = rawdata.randomSplit(weights=[0.7, 0.1, 0.2], seed=1234)\n",
    "                    sigma = fsigma * np.sqrt(dimensions)\n",
    "\n",
    "                if kdataset in [6]: # loading as text, transforming variables, converting to labelledpoint and splitting RDD into train, val and test\n",
    "                    if kdataset == 6:\n",
    "                        filename = 'file:///export/g2pi/SPARK/data/kddcup1999_tr'\n",
    "                        XtrRDD = sc.textFile(filename)\n",
    "                        XtrRDD = XtrRDD.map(SVM_UTILS.text2labeled)\n",
    "\n",
    "                        filename = 'file:///export/g2pi/SPARK/data/kddcup1999_val'\n",
    "                        XvalRDD = sc.textFile(filename)\n",
    "                        XvalRDD = XvalRDD.map(SVM_UTILS.text2labeled)\n",
    "\n",
    "                        filename = 'file:///export/g2pi/SPARK/data/kddcup1999_tst'\n",
    "                        XtstRDD = sc.textFile(filename)\n",
    "                        XtstRDD = XtstRDD.map(SVM_UTILS.text2labeled)\n",
    "\n",
    "                        XtrRDD.cache()\n",
    "                        XvalRDD.cache()\n",
    "                        XtstRDD.cache()\n",
    "                        sigma = fsigma * np.sqrt(len(XtstRDD.take(1)[0].features))\n",
    "\n",
    "                if kdataset in [7]: # loading as text, transforming to labelledpoint\n",
    "                    if kdataset == 7:\n",
    "                        \n",
    "                        filename = 'file:///export/g2pi/SPARK/data/higgs_tr.txt'\n",
    "                        XtrRDD = sc.textFile(filename)\n",
    "                        XtrRDD = XtrRDD.map(SVM_UTILS.text2labeled)\n",
    "\n",
    "                        filename = 'file:///export/g2pi/SPARK/data/higgs_val.txt'\n",
    "                        XvalRDD = sc.textFile(filename)\n",
    "                        XvalRDD = XvalRDD.map(SVM_UTILS.text2labeled)\n",
    "                                                \n",
    "                        filename = 'file:///export/g2pi/SPARK/data/higgs_tst.txt'\n",
    "                        XtstRDD = sc.textFile(filename)\n",
    "                        XtstRDD = XtstRDD.map(SVM_UTILS.text2labeled)\n",
    "                        sigma = fsigma * np.sqrt(len(XtstRDD.take(1)[0].features))\n",
    "\n",
    "                        XtrRDD.cache()\n",
    "                        XvalRDD.cache()\n",
    "                        XtstRDD.cache()\n",
    "\n",
    "                #################################   END LOADING DATA ##########################################################\n",
    "                \n",
    "                for NC in NCs:\n",
    "                    print \"Dataset = %s, modelo = %s, kfold = %d, Niter = %d, NC = %d\" % (name_dataset, modelo, kfold, Niter, NC)\n",
    "                    filename = './results/dataset_' + str(kdataset) + '_modelo_' + modelo + '_NC_' + str(NC) + '_Niter_' + str(Niter) + '_kfold_' + str(kfold) + '.pkl'\n",
    "                    #import code\n",
    "                    #code.interact(local=locals())\n",
    "                    try:\n",
    "                        f = open(filename,'r')\n",
    "                        f.close()\n",
    "                        file_exists = True\n",
    "                    except:\n",
    "                        file_exists = False\n",
    "                        pass\n",
    "                    execute = False\n",
    "                    if file_exists:\n",
    "                        if overwrite_results:\n",
    "                            execute = True\n",
    "                    else:\n",
    "                        execute = True                                  \n",
    "                    if execute:\n",
    "\n",
    "                        if modelo == 'hybridgrad':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time = SVM_UTILS.train_hybridSVM(XtrRDD, XvalRDD, XtstRDD, sigma, C, NC, Niter, Samplefraction)\n",
    "\n",
    "                        if modelo == 'kernelgrad':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time = SVM_UTILS.train_kernelgrad(XtrRDD, XvalRDD, XtstRDD, sigma, C, NC, Niter, Samplefraction)\n",
    "\n",
    "                        if modelo == 'SGMA_IRWLS':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time = train_SGMA_IRWLS(XtrRDD, XvalRDD, XtstRDD, sigma, C, NC, Niter)\n",
    "\n",
    "                        if modelo == 'LinearSVM':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time = SVM_UTILS.train_linear_SVM(XtrRDD, XvalRDD, XtstRDD)\n",
    "                                                \n",
    "                        if modelo == 'Logistic':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time = SVM_UTILS.train_logistic(XtrRDD, XvalRDD, XtstRDD)\n",
    "\n",
    "                        if modelo == 'random_IRWLS':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time = train_random_IRWLS(XtrRDD, XvalRDD, XtstRDD, sigma, C, NC, Niter)\n",
    "\n",
    "                        if modelo == 'hybrid_IRWLS':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time = train_hybrid_SGMA_IRWLS(XtrRDD, XvalRDD, XtstRDD, sigma, C, NC, Niter)\n",
    "\n",
    "                        if modelo == 'Kmeans_IRWLS':\n",
    "                            auc_tr, auc_val, auc_tst, exe_time = train_kmeans_IRWLS(XtrRDD, XvalRDD, XtstRDD, sigma, C, NC, Niter)\n",
    "\n",
    "                        print \"Dataset = %s, modelo = %s, kfold = %d, Niter = %d, NC = %d\" % (name_dataset, modelo, kfold, Niter, NC)\n",
    "                        print \"AUCtr = %f, AUCval = %f, AUCtst = %f\" % (auc_tr, auc_val, auc_tst)\n",
    "                        print \"Elapsed minutes = %f\" % (exe_time / 60.0)\n",
    "\n",
    "                        with open(filename, 'w') as f:\n",
    "                            pickle.dump([auc_tr, auc_val, auc_tst, exe_time], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
